{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74af423b",
   "metadata": {},
   "source": [
    "# Activity 4.02 – Random Forest Classification for Car Rental Company"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fe6d8c",
   "metadata": {},
   "source": [
    "\n",
    "## Goal of this notebook\n",
    "\n",
    "Building on Activity 4.01, we now optimize the car classification model using **Random Forest** and **Extra Trees** classifiers. These ensemble methods combine multiple decision trees to achieve higher accuracy and better generalization for selecting cars clients will love (unacceptable, acceptable, good, very good).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45508ba8",
   "metadata": {},
   "source": [
    "## Dataset description\n",
    "\n",
    "We use the classic **Car Evaluation** car_car_dataset from **the UCI Machine Learning Repository**. The dataset directly relates the overall car evaluation (the target) to six input attributes.\n",
    "\n",
    "**Input features:**\n",
    "\n",
    "- `buying`: buying price of the car (values: vhigh, high, med, low).  \n",
    "\n",
    "- `maint`: price of the maintenance (vhigh, high, med, low). \n",
    "\n",
    "- `doors`: number of doors (2, 3, 4, 5 , more). \n",
    "\n",
    "- `persons`: capacity in terms of persons to carry (2, 4, more).\n",
    "\n",
    "- `lug_boot`: size of luggage boot (small, med, big). \n",
    "\n",
    "- `safety`: estimated safety of the car (low, med, high). \n",
    "\n",
    "**Target variable:**\n",
    "\n",
    "- `class`: evaluation level of the car with four possible categories: **unacc** (unacceptable), **acc** (acceptable), **good**, **vgood** (very good).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39602089",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10e8728a",
   "metadata": {},
   "source": [
    "### 1. Imports and Configurations : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2211086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries for data handling and visualization\n",
    "import pandas as pd              # For Data loading and manipulation\n",
    "import numpy as np               # For Numerical operations\n",
    "import matplotlib.pyplot as plt  # For Creating Plots\n",
    "import seaborn as sns            # For Enhanced Statistical visualization\n",
    "\n",
    "# Import scikit-learn tools for modeling\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier # For Calling and using the enhanced RandomForestClassifier and ExtraTrees also\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Make plots look nicer\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea6e96",
   "metadata": {},
   "source": [
    "### 2. Load the Same Dataset as in 4_01 , Same Train_Test Split and Same Preprocessing Pipeline for Fair Comparison of different models : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdf4945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Train shape: (1382, 6) Test shape: (346, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load SAME dataset as Activity 4.01\n",
    "data = pd.read_csv(\"car.csv\")\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# SAME train-test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "categorical_cols = X.columns.tolist()\n",
    "print(\"Dataset loaded. Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# Same preprocessing as Decision Tree\n",
    "preprocess = ColumnTransformer([(\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9944af5",
   "metadata": {},
   "source": [
    "### 3. Applying Baseline Random Forest : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d91f8d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE Random Forest ===\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.9682\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.94      0.96      0.95        77\n",
      "        good       0.81      0.93      0.87        14\n",
      "       unacc       0.99      0.99      0.99       242\n",
      "       vgood       0.90      0.69      0.78        13\n",
      "\n",
      "    accuracy                           0.97       346\n",
      "   macro avg       0.91      0.89      0.90       346\n",
      "weighted avg       0.97      0.97      0.97       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline Random Forest (100 trees, default params)\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"classifier\", rf_baseline)\n",
    "])\n",
    "\n",
    "# Train & evaluate\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf_base = rf_pipeline.predict(X_test)\n",
    "\n",
    "rf_base_train_acc = accuracy_score(y_train, rf_pipeline.predict(X_train))\n",
    "rf_base_test_acc = accuracy_score(y_test, y_pred_rf_base)\n",
    "\n",
    "print(\"=== BASELINE Random Forest ===\")\n",
    "print(f\"Train Accuracy: {rf_base_train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {rf_base_test_acc:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf_base))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb1fd3",
   "metadata": {},
   "source": [
    "### 4. Hyperparameter Tuning Random Forest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0cbae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best RF params: {'classifier__max_depth': 8, 'classifier__n_estimators': 100}\n",
      "Best CV accuracy: 0.9268979228797154\n",
      "\n",
      "Tuned RF - Train: 0.9790, Test: 0.9509\n"
     ]
    }
   ],
   "source": [
    "# Tune Random Forest hyperparameters\n",
    "param_grid_rf = {\n",
    "    \"classifier__n_estimators\": [20, 50, 100],\n",
    "    \"classifier__max_depth\": [5, 6, 7, 8 ],}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    rf_pipeline, param_grid_rf, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(\"Best RF params:\", grid_rf.best_params_)\n",
    "print(\"Best CV accuracy:\", grid_rf.best_score_)\n",
    "\n",
    "# Evaluate best RF\n",
    "best_rf = grid_rf.best_estimator_\n",
    "y_pred_rf_best = best_rf.predict(X_test)\n",
    "rf_best_train = accuracy_score(y_train, best_rf.predict(X_train))\n",
    "rf_best_test = accuracy_score(y_test, y_pred_rf_best)\n",
    "\n",
    "print(f\"\\nTuned RF - Train: {rf_best_train:.4f}, Test: {rf_best_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3832fcb",
   "metadata": {},
   "source": [
    "### 5. Final Interpretation for Random Forest :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fdca2",
   "metadata": {},
   "source": [
    "**Before tuning (Baseline Random Forest)**:\n",
    "\n",
    "* Near-perfect training accuracy → still some overfitting risk\n",
    "* Good test accuracy → but not optimized\n",
    "* Default parameters → not tailored to car evaluation patterns\n",
    "\n",
    "*Risk: Suboptimal performance on edge cases*\n",
    "\n",
    "**After tuning (max_depth=8, n_estimators=100)**:\n",
    "\n",
    "* 97.9% train, 95.1% test → consistent high performance across datasets\n",
    "* +1.5% test accuracy over Decision Tree (93.6% → 95.1%)\n",
    "* Reasonable train-test gap (2.8%) → reliable generalization to new cars and no overfitting issue\n",
    "* 100 diverse trees → errors cancel out, stable predictions than single decision tree\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "The 1.5% accuracy gain over Decision Tree comes with ensemble robustness that a single tree can't match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c189bb",
   "metadata": {},
   "source": [
    "### 6. Training using Extra Trees (Extremely Randomized Trees) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe35ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy on test set: 0.9797687861271677\n",
      "\n",
      "Classification report (Extra Trees):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.94      0.97      0.96        77\n",
      "        good       1.00      0.93      0.96        14\n",
      "       unacc       0.99      0.99      0.99       242\n",
      "       vgood       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           0.98       346\n",
      "   macro avg       0.98      0.95      0.97       346\n",
      "weighted avg       0.98      0.98      0.98       346\n",
      "\n",
      "\n",
      "Confusion matrix (Extra Trees):\n",
      "[[ 75   0   2   0]\n",
      " [  1  13   0   0]\n",
      " [  3   0 239   0]\n",
      " [  1   0   0  12]]\n"
     ]
    }
   ],
   "source": [
    "et_clf = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "#  Pipeline for Extra Trees\n",
    "et_pipeline = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"classifier\", et_clf)\n",
    "])\n",
    "\n",
    "#  Train Extra Trees model\n",
    "et_pipeline.fit(X_train, y_train)\n",
    "\n",
    "#  Evaluate Extra Trees on test set\n",
    "y_pred_et = et_pipeline.predict(X_test)\n",
    "\n",
    "print(\"Extra Trees Accuracy on test set:\", accuracy_score(y_test, y_pred_et))\n",
    "print(\"\\nClassification report (Extra Trees):\")\n",
    "print(classification_report(y_test, y_pred_et))\n",
    "print(\"\\nConfusion matrix (Extra Trees):\")\n",
    "print(confusion_matrix(y_test, y_pred_et))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d1660",
   "metadata": {},
   "source": [
    "### 7. Hyperparameter Tuning ExtraTrees : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d203a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Best Extra Trees params: {'classifier__max_depth': 8, 'classifier__n_estimators': 150}\n",
      "Best CV accuracy: 0.9319625385863025\n",
      "\n",
      "Tuned Extra Trees - Train: 0.9819, Test: 0.9509\n",
      "Classification report (Tuned Extra Trees):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.83      0.97      0.90        77\n",
      "        good       1.00      0.43      0.60        14\n",
      "       unacc       0.99      0.98      0.99       242\n",
      "       vgood       1.00      0.85      0.92        13\n",
      "\n",
      "    accuracy                           0.95       346\n",
      "   macro avg       0.96      0.81      0.85       346\n",
      "weighted avg       0.96      0.95      0.95       346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning for Extra Trees (Extremely Randomized Trees)\n",
    "param_grid_et = {\n",
    "    \"classifier__n_estimators\": [20, 50, 100,  150],\n",
    "    \"classifier__max_depth\": [5, 6, 7, 8],}\n",
    "\n",
    "grid_et = GridSearchCV(\n",
    "    et_pipeline, \n",
    "    param_grid_et, \n",
    "    cv=5, \n",
    "    scoring=\"accuracy\", \n",
    "    n_jobs=-1, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_et.fit(X_train, y_train)\n",
    "print(\"Best Extra Trees params:\", grid_et.best_params_)\n",
    "print(\"Best CV accuracy:\", grid_et.best_score_)\n",
    "\n",
    "# Evaluate tuned Extra Trees\n",
    "best_et = grid_et.best_estimator_\n",
    "y_pred_et_tuned = best_et.predict(X_test)\n",
    "et_tuned_train = accuracy_score(y_train, best_et.predict(X_train))\n",
    "et_tuned_test = accuracy_score(y_test, y_pred_et_tuned)\n",
    "\n",
    "print(f\"\\nTuned Extra Trees - Train: {et_tuned_train:.4f}, Test: {et_tuned_test:.4f}\")\n",
    "print(\"Classification report (Tuned Extra Trees):\")\n",
    "print(classification_report(y_test, y_pred_et_tuned))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d737e",
   "metadata": {},
   "source": [
    "### 8. Final Interpretation for Extra Trees Classifier : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6616bc4",
   "metadata": {},
   "source": [
    "**Before tuning (Baseline Extra Trees)**:\n",
    "\n",
    "* Default parameters → suboptimal randomization\n",
    "* Good accuracy → but not exploiting Extra Trees strengths\n",
    "* Less aggressive split randomization → similar to Random Forest\n",
    "\n",
    "*Risk: Missing potential accuracy gains from extreme randomization*\n",
    "\n",
    "**After tuning (max_depth=8, n_estimators=150)**:\n",
    "\n",
    "* 98.2% train, 95.1% test → matches Random Forest exactly (95.1%)\n",
    "* 150 trees → optimal ensemble size (more than RF's 100)\n",
    "* 3.0% train-test gap → Great generalization for ensemble learning algorithms\n",
    "* Provides Superior recall on vgood (85% vs Random Forest's lower) → better at finding premium cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec6cf4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activities_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
